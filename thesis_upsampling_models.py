# -*- coding: utf-8 -*-
"""Thesis_upsampling_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/102N0ihWJitwume9E5OpByZklqIbALPb_
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""## Loading kaggle dataset """

! pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download andyczhao/covidx-cxr2

! unzip covidx-cxr2

"""## Loading packages"""

from tensorflow.keras.layers import Input, Dropout, Conv2D, BatchNormalization, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.mobilenet import preprocess_input
from tensorflow.keras.applications.mobilenet  import decode_predictions
from tensorflow.keras.applications.mobilenet import MobileNet
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
import numpy as np
import pandas as pd
from glob import glob
import matplotlib.pyplot as plt
import tensorflow as tf
import random
import cv2
import matplotlib.image as mpimg
from PIL import Image, ImageOps
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, BatchNormalization
from tensorflow.keras.optimizers import Adam,RMSprop
from tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D, MaxPooling2D
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Flatten, Dropout, Input, Lambda, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers.experimental import preprocessing

## Setting options for the random seed
random.seed(123)
tf.random.set_seed(123)
np.random.seed(123)

## Loading training set as dataframe
train_df = pd.read_csv("/content/train.txt", sep=" ", header=None)
train_df.columns=['patient id', 'file_paths', 'labels', 'data source']
train_df=train_df.drop(['patient id', 'data source'], axis=1 )
train_df.head()

## Loading test set as dataframe
test_df = pd.read_csv("/content/test.txt", sep=" ", header=None)
test_df.columns=['id', 'file_paths', 'labels', 'data source' ]
test_df=test_df.drop(['id', 'data source'], axis=1 )
test_df.head()

## Setting directory paths for train and test
train_path = "/content/train"  
test_dir = "/content/test"

## Counting training labels
train_df['labels'].value_counts()

## Counting test labels
test_df['labels'].value_counts()

## Splitting the train data into train and validation data (70:30 ratio)
train_df, valid_df = train_test_split(train_df, train_size=0.7, random_state=0, stratify=train_df.labels)

## Printing the train, validation and test cases (before the sampling method)
print(train_df.labels.value_counts())
print(valid_df.labels.value_counts())
print(test_df.labels.value_counts())

## Label conversion of the train data
negative  = train_df[train_df['labels']=='negative'] 
positive = train_df[train_df['labels']=='positive']

## Resampling of the dataset after the split
from sklearn.utils import resample

df_majority_downsampled = resample(negative, replace = True, 
                                   n_samples = 11543)
train_df = pd.concat([positive, df_majority_downsampled])

from sklearn.utils import shuffle
train_df = shuffle(train_df)

## Printing the train, validation and test cases (after the sampling method)
print(train_df.labels.value_counts())
print(valid_df.labels.value_counts())
print(test_df.labels.value_counts())

## Setting the parameters for the models
IMAGE_SIZE    = (224, 224)
NUM_CLASSES   = 2
BATCH_SIZE    = 64  
NUM_EPOCHS    = 10

## Setting up an imagegenerator for data augmentation
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=10,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                
                                   validation_split = 0.1,
                                   horizontal_flip = True ,
                                   vertical_flip = False,
                                   fill_mode='nearest')

# Using the abovementioned image generator on the train and validation set
train_batches = train_datagen.flow_from_dataframe(dataframe = train_df, directory=train_path, x_col='file_paths', 
                                              y_col='labels',
                                                  target_size=IMAGE_SIZE,
                                                  subset = "training",
                                                  shuffle=True,
                                                  batch_size=BATCH_SIZE,
                                                  class_mode="binary"  
                                                  )
valid_batches = train_datagen.flow_from_dataframe(dataframe = valid_df, directory=train_path, x_col='file_paths',
                                             y_col='labels',
                                            subset = "validation",
                                                  target_size=IMAGE_SIZE,
                                                  shuffle=True,
                                                  batch_size=BATCH_SIZE,
                                                  class_mode="binary"  
                                                  )

"""## Transfer Learning with MobileNet


"""

## Creating the basemodel with on the ImageNet weights
baseModel =MobileNet(weights= 'imagenet', include_top = False, input_shape=(224, 224, 3))
for layer in baseModel.layers[:-2]:
      layer.trainable = False

## Creating the model to train
headModel = baseModel.output
headModel = BatchNormalization()(headModel)
#headModel = GlobalAveragePooling2D()(headModel)
headModel = Flatten()(headModel)
#headModel = BatchNormalization()(headModel)
headModel = Dense(4096, activation="relu")(headModel)
headModel = BatchNormalization()(headModel)
headModel = Dropout(0.1)(headModel)
headModel = Dense(1024, activation="relu")(headModel)
#headModel = BatchNormalization()(headModel)
headModel = Dropout(0.2)(headModel)
headModel = Dense(1, activation="sigmoid")(headModel)
model = Model(inputs=baseModel.input, outputs=headModel)
optimizers = RMSprop(learning_rate=  0.0001)
model.compile(loss = 'binary_crossentropy', optimizer = optimizers, metrics = ['accuracy'])

## Getting the model summary for the model
model.summary()

from keras import callbacks
import os

filepath = "/content/best_mobilenet.hfd5"
checkpoint = callbacks.ModelCheckpoint( filepath, monitor = 'val_loss', save_best_only = True, 
                                       mode = 'min', verbose = 1)
callbacks_list = [checkpoint]

import datetime
import keras

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = keras.callbacks.TensorBoard(logdir)

## Fitting the model
print(len(train_batches))
print(len(valid_batches))

STEP_SIZE_TRAIN=train_batches.n//train_batches.batch_size
STEP_SIZE_VALID=valid_batches.n//valid_batches.batch_size

result=model.fit(train_batches,
                        steps_per_epoch =STEP_SIZE_TRAIN, epochs= NUM_EPOCHS,
                        validation_data = valid_batches,
                        validation_steps = STEP_SIZE_VALID,
                        shuffle=True, verbose=1,callbacks = [tensorboard_callback, checkpoint]  )

model.load_weights(r'/content/logs/best_mobilenet.hfd5')

## Plotting the loss versus accuracy for validation

acc = result.history['accuracy'] 
loss = result.history['loss']
val_loss = result.history['val_loss']
val_acc = result.history['val_accuracy']
epochs = range(len(result.epoch))

title1 = 'Accuracy vs Validation Accuracy'
leg1 = ['Acc', 'Val_acc']
title2 = 'Loss vs Val_loss'
leg2 = ['Loss', 'Val_loss']

def plot(epochs, acc, val_acc, leg, title):
    plt.plot(epochs, acc)
    plt.plot(epochs, val_acc)
    plt.title(title)
    plt.legend(leg)
    plt.xlabel('epochs')

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plot(epochs, acc, val_acc, leg1, title1)
plt.subplot(1,2,2)
plot(epochs, loss, val_loss, leg2, title2)
plt.show()

save_dir=r"/content/mobilenet"
subject='RetinaValid'
model_name='Densenet'
save_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')
save_loc=os.path.join(save_dir, save_id)
model.save(save_loc)

## Data augmentation for the test set
test_datagen = ImageDataGenerator(rescale=1./255)
eval_generator = test_datagen.flow_from_dataframe(test_df, directory = test_dir,  x_col='file_paths', y_col='labels', target_size=IMAGE_SIZE, class_mode='binary',
                                    color_mode='rgb', shuffle=False, batch_size=64)
eval_generator.reset()
#Evalute the trained model on evaluate generator
eval_generator.reset()  
x = model.evaluate(eval_generator,
                           steps = np.ceil(len(eval_generator)), 
                           use_multiprocessing = False,  verbose = 1,
                           workers=1)
 
print('Test loss:' , x[0])
print('Test accuracy:',x[1])

## Testing the model
pred = model.predict(eval_generator)

len(pred)

preds=pred.round()

## Confusion matrix for the test performance
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
print('Confusion Matrix')
print(confusion_matrix(eval_generator.classes, preds))
cm = confusion_matrix(eval_generator.classes, preds )
print('Classification Report')
target_names = ['Positive', 'Negative']
print(classification_report(eval_generator.classes, preds, target_names=target_names))

ConfusionMatrixDisplay.from_predictions(eval_generator.classes, preds,
                                       cmap=plt.cm.Blues)

"""## Transfer Learning with Densenet121 """

from tensorflow.keras.applications import DenseNet121

basemodel_d =DenseNet121(weights= 'imagenet', include_top = False, input_shape=(224, 224, 3))
for layer in basemodel_d.layers[:-2]:
      layer.trainable = False

headmodel_d = basemodel_d.output
headmodel_d = BatchNormalization()(headmodel_d)
#headmodel_d = GlobalAveragePooling2D()(headmodel_d)
headmodel_d = Flatten()(headmodel_d)
#headmodel_d = BatchNormalization()(headmodel_d)
headmodel_d = Dense(4096, activation="relu")(headmodel_d)
headmodel_d = BatchNormalization()(headmodel_d)
headmodel_d = Dropout(0.1)(headmodel_d)
headmodel_d = Dense(1024, activation="relu")(headmodel_d)
#headmodel_d = BatchNormalization()(headmodel_d)
headmodel_d = Dropout(0.2)(headmodel_d)
headmodel_d = Dense(1, activation="sigmoid")(headmodel_d)
model_d = Model(inputs=basemodel_d.input, outputs=headmodel_d)
optimizers = RMSprop(learning_rate=  0.0001)
model_d.compile(loss = 'binary_crossentropy', optimizer = optimizers, metrics = ['accuracy'])

model_d.summary()

from keras import callbacks
import os

filepath = "/content/best_densenet.hfd5"
checkpoint_d = callbacks.ModelCheckpoint( filepath, monitor = 'val_loss', save_best_only = True, 
                                       mode = 'min', verbose = 1)
callbacks_list = [checkpoint_d]

import datetime
import keras

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback_d = keras.callbacks.TensorBoard(logdir)

result_d=model_d.fit(train_batches,
                        steps_per_epoch =STEP_SIZE_TRAIN, epochs= NUM_EPOCHS,
                        validation_data = valid_batches,
                        validation_steps = STEP_SIZE_VALID,
                        shuffle=True, verbose=1,callbacks = [tensorboard_callback_d, checkpoint_d]  )



test_datagen = ImageDataGenerator(rescale=1./255)
eval_generator = test_datagen.flow_from_dataframe(test_df, directory = test_dir,  x_col='file_paths', y_col='labels', target_size=IMAGE_SIZE, class_mode='binary',
                                    color_mode='rgb', shuffle=False, batch_size=64)
eval_generator.reset()
#Evaluate the trained model on evaluate generator
eval_generator.reset()  
x = model_d.evaluate(eval_generator,
                           steps = np.ceil(len(eval_generator)), 
                           use_multiprocessing = False,  verbose = 1,
                           workers=1)
 
print('Test loss:' , x[0])
print('Test accuracy:',x[1])

pred_d = model_d.predict(eval_generator)

len(pred_d)

preds_d=pred_d.round()

# Plotting the loss versus accuracy

acc = result_d.history['accuracy'] 
loss = result_d.history['loss']
val_loss = result_d.history['val_loss']
val_acc = result_d.history['val_accuracy']
epochs = range(len(result_d.epoch))

title1 = 'Accuracy vs Validation Accuracy'
leg1 = ['Acc', 'Val_acc']
title2 = 'Loss vs Val_loss'
leg2 = ['Loss', 'Val_loss']

def plot(epochs, acc, val_acc, leg, title):
    plt.plot(epochs, acc)
    plt.plot(epochs, val_acc)
    plt.title(title)
    plt.legend(leg)
    plt.xlabel('epochs')

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plot(epochs, acc, val_acc, leg1, title1)
plt.subplot(1,2,2)
plot(epochs, loss, val_loss, leg2, title2)
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
print('Confusion Matrix')
print(confusion_matrix(eval_generator.classes, preds_d))
cm = confusion_matrix(eval_generator.classes, preds_d )
print('Classification Report')
target_names = ['Positive', 'Negative']
print(classification_report(eval_generator.classes, preds_d, target_names=target_names))

ConfusionMatrixDisplay.from_predictions(eval_generator.classes, preds_d,
                                       cmap=plt.cm.Blues)

"""## Transfer Learning with VGG16"""

from keras.applications.vgg16 import VGG16

basemodel_v =VGG16(weights= 'imagenet', include_top = False, input_shape=(224, 224, 3))
for layer in basemodel_v.layers[:-2]:
      layer.trainable = False

headmodel_v = basemodel_v.output
headmodel_v = BatchNormalization()(headmodel_v)
#headmodel_v = GlobalAveragePooling2D()(headmodel_v)
headmodel_v = Flatten()(headmodel_v)
#headmodel_v = BatchNormalization()(headmodel_v)
headmodel_v = Dense(4096, activation="relu")(headmodel_v)
headmodel_v = BatchNormalization()(headmodel_v)
headmodel_v = Dropout(0.1)(headmodel_v)
headmodel_v = Dense(1024, activation="relu")(headmodel_v)
#headmodel_v = BatchNormalization()(headmodel_v)
headmodel_v = Dropout(0.2)(headmodel_v)
headmodel_v = Dense(1, activation="sigmoid")(headmodel_v)
model_v = Model(inputs=basemodel_v.input, outputs=headmodel_v)
optimizers = RMSprop(learning_rate=  0.0001)
model_v.compile(loss = 'binary_crossentropy', optimizer = optimizers, metrics = ['accuracy'])

model_v.summary()

filepath = "/content/vgg16.hfd5"
checkpoint_v = callbacks.ModelCheckpoint( filepath, monitor = 'val_loss', save_best_only = True, 
                                       mode = 'min', verbose = 1)
callbacks_list = [checkpoint_v]

logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback_v = keras.callbacks.TensorBoard(logdir)

result_v=model_v.fit(train_batches,
                        steps_per_epoch =STEP_SIZE_TRAIN, epochs= NUM_EPOCHS,
                        validation_data = valid_batches,
                        validation_steps = STEP_SIZE_VALID,
                        shuffle=True, verbose=1,callbacks = [tensorboard_callback_v, checkpoint_v]  )



test_datagen = ImageDataGenerator(rescale=1./255)
eval_generator = test_datagen.flow_from_dataframe(test_df, directory = test_dir,  x_col='file_paths', y_col='labels', target_size=IMAGE_SIZE, class_mode='binary',
                                    color_mode='rgb', shuffle=False, batch_size=64)
eval_generator.reset()
#Evalute the trained model on evaluate generator
eval_generator.reset()  
x = model_v.evaluate(eval_generator,
                           steps = np.ceil(len(eval_generator)), 
                           use_multiprocessing = False,  verbose = 1,
                           workers=1)
 
print('Test loss:' , x[0])
print('Test accuracy:',x[1])

pred_v = model_v.predict(eval_generator)

len(pred_v)

preds_v=pred_v.round()

# van kaggle

acc = result_v.history['accuracy'] 
loss = result_v.history['loss']
val_loss = result_v.history['val_loss']
val_acc = result_v.history['val_accuracy']
epochs = range(len(result_v.epoch))

title1 = 'Accuracy vs Validation Accuracy'
leg1 = ['Acc', 'Val_acc']
title2 = 'Loss vs Val_loss'
leg2 = ['Loss', 'Val_loss']

def plot(epochs, acc, val_acc, leg, title):
    plt.plot(epochs, acc)
    plt.plot(epochs, val_acc)
    plt.title(title)
    plt.legend(leg)
    plt.xlabel('epochs')

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plot(epochs, acc, val_acc, leg1, title1)
plt.subplot(1,2,2)
plot(epochs, loss, val_loss, leg2, title2)
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
print('Confusion Matrix')
print(confusion_matrix(eval_generator.classes, preds_v))
cm = confusion_matrix(eval_generator.classes, preds_v )
print('Classification Report')
target_names = ['Positive', 'Negative']
print(classification_report(eval_generator.classes, preds_v, target_names=target_names))

ConfusionMatrixDisplay.from_predictions(eval_generator.classes, preds_v,
                                       cmap=plt.cm.Blues)

"""##Source codes
https://www.kaggle.com/sohommajumder21/resnet-covidx-beginner-friendly-codes-explained

https://www.kaggle.com/arpithaananth/covid-x-ray-image-classification-with-efficientnet

https://github.com/abeerbadawi/COVID-ChestXray15k-Dataset-Transfer-learning/blob/main/Detecting%20Coronavirus%20from%20Chest%20X-rays%20Using%20Transfer%20Learning.ipynb

https://www.kaggle.com/vexxingbanana/resnet50v2-test-acc-97


"""